{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gIPWi-HihXy",
        "outputId": "9bb3b8dd-78b6-43b4-c151-beec207e0271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data={\"intents\":[\n",
        "    {\n",
        "      \"tag\":\"greeting\",\n",
        "      \"patterns\":[\"Hi\",\"Hello\",\"Hey\",\"How are u?\"],\n",
        "      \"responses\":[\"How are you doing?\",\"Hello\",\"Greetings!\",\"Howdy partner!\"]\n",
        "    },\n",
        "    {\n",
        "        \"tag\":\"age\",\n",
        "        \"patterns\":[\"how old are you\",\"when were you born\",\"when is your birthday\",\"how old are u\"],\n",
        "        \"responses\":[\"I am 20 years old\",\"I was born in 2002\",\"My birthday is June 29th and I was born in 2002\",\"29/06/2002\"]\n",
        "    },\n",
        "    {\n",
        "        \"tag\":\"name\",\n",
        "        \"patterns\":[\"What's your name?\",\"who are you\",\"whats ur name\"],\n",
        "        \"responses\":[\"My name is John\",\"I'm John\",\"John\"]\n",
        "        \n",
        "        \n",
        "    },\n",
        "    {\n",
        "        \"tag\":\"goodbye\",\n",
        "        \"patterns\":[\"bye\",\"later\",\"cya\"],\n",
        "        \"responses\":[\"Bye\",\"see you later\"]\n",
        "                                                    \n",
        "    \n",
        "    },\n",
        "    {\n",
        "        \"tag\": \"date\",\n",
        "        \"patterns\":[\"what are you doing this weekend\",\"do you want to hangout sometime?\",\"what are your plans for this week\"],\n",
        "        \"responses\":[\"I am available all week\",\"I don't have any plans\",\"I am not busy\"]\n",
        "    },\n",
        "    {\n",
        "        \"tag\":\"funny\",\n",
        "        \"patterns\":[\"Tell me a joke!\",\"Tell me something funny!\",\"Do you know a joke?\"],\n",
        "        \"responses\":[\"What did the buffalo say when his son left for college? Bison\",\"Why did the hipster burn his mouth? He drank the coffee before it was cool.\"]\n",
        "    }\n",
        "      ]}"
      ],
      "metadata": {
        "id": "iq3_ZR3_iswk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm=WordNetLemmatizer() #for getting words\n",
        "#lists\n",
        "classes=[]\n",
        "newWords=[]\n",
        "doc_X=[]\n",
        "doc_Y=[]\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        tokens=nltk.word_tokenize(pattern)#tokenize the patterns\n",
        "        newWords.extend(tokens)# extend the tokens\n",
        "        doc_X.append(pattern)\n",
        "        doc_Y.append(intent[\"tag\"])\n",
        "        \n",
        "    if intent[\"tag\"] not in classes:\n",
        "        classes.append(intent[\"tag\"])\n",
        "        \n",
        "newWords=[lm.lemmatize(word.lower()) for word in newWords if word not in string.punctuation] #setting words to lowercase if not in punctuation\n",
        "newWords=sorted(set(newWords))\n",
        "classes=sorted(set(classes))"
      ],
      "metadata": {
        "id": "XUlR4zcgiy3k"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classes\n",
        "     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5EL4Zdki-Rq",
        "outputId": "5490d6eb-3cdd-4a54-a4ff-ad49ee2d9d37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['age', 'date', 'funny', 'goodbye', 'greeting', 'name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(newWords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPI97ciVjNAL",
        "outputId": "76543fd5-d6b8-4f8e-f16f-3ccb3c8593e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'s\", 'a', 'are', 'birthday', 'born', 'bye', 'cya', 'do', 'doing', 'for', 'funny', 'hangout', 'hello', 'hey', 'hi', 'how', 'is', 'joke', 'know', 'later', 'me', 'name', 'old', 'plan', 'something', 'sometime', 'tell', 'this', 'to', 'u', 'ur', 'want', 'week', 'weekend', 'were', 'what', 'whats', 'when', 'who', 'you', 'your']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-hG6aHEjPpZ",
        "outputId": "35119a41-f905-444d-e6f8-d601c58c68fb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'Hello', 'Hey', 'How are u?', 'how old are you', 'when were you born', 'when is your birthday', 'how old are u', \"What's your name?\", 'who are you', 'whats ur name', 'bye', 'later', 'cya', 'what are you doing this weekend', 'do you want to hangout sometime?', 'what are your plans for this week', 'Tell me a joke!', 'Tell me something funny!', 'Do you know a joke?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJAxcbm6jR5y",
        "outputId": "ddaa0f31-e4cf-4dbf-b30f-3a1df8520b8a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['greeting', 'greeting', 'greeting', 'greeting', 'age', 'age', 'age', 'age', 'name', 'name', 'name', 'goodbye', 'goodbye', 'goodbye', 'date', 'date', 'date', 'funny', 'funny', 'funny']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainingdata=[] #training list array\n",
        "outEmpty=[0]*len(classes)\n",
        "for idx,doc in enumerate(doc_X):\n",
        "    bagOfwords=[]\n",
        "    text=lm.lemmatize(doc.lower())\n",
        "    for word in newWords:\n",
        "        bagOfwords.append(1) if word in text else bagOfwords.append(0)\n",
        "    outputRow=list(outEmpty)\n",
        "    outputRow[classes.index(doc_Y[idx])]=1\n",
        "    trainingdata.append([bagOfwords,outputRow])\n",
        "random.shuffle(trainingdata)\n",
        "trainingdata=np.array(trainingdata,dtype=object)\n",
        "train_x=np.array(list(trainingdata[:,0]))\n",
        "train_y=np.array(list(trainingdata[:,1]))\n",
        "   "
      ],
      "metadata": {
        "id": "06kqxn5ujWIl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ishape=(len(train_x[0]),)\n",
        "oshape=len(train_y[0])\n",
        "epochs=200\n",
        "model=Sequential()\n",
        "model.add(Dense(128,input_shape=ishape,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(oshape,activation=\"softmax\"))\n",
        "md=tf.keras.optimizers.Adam(learning_rate=0.01,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=md,\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary()) #output the model in summary\n",
        "model.fit(x=train_x,y=train_y,epochs=200,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu3xlmvKjXaC",
        "outputId": "1462eeac-b9eb-4108-e178-a4d89cb02401"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               5376      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,022\n",
            "Trainable params: 14,022\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 780ms/step - loss: 1.7626 - accuracy: 0.2000\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6283 - accuracy: 0.3500\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6691 - accuracy: 0.3000\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4102 - accuracy: 0.6000\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4202 - accuracy: 0.6000\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1836 - accuracy: 0.7000\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2025 - accuracy: 0.7000\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0698 - accuracy: 0.6500\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0745 - accuracy: 0.7500\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7468 - accuracy: 0.9500\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6098 - accuracy: 0.9500\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6199 - accuracy: 0.8500\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4876 - accuracy: 0.8500\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4533 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3285 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3072 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2439 - accuracy: 0.9500\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2094 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2944 - accuracy: 0.8500\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1044 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0406 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1077 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0811 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2922 - accuracy: 0.8000\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0821 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0790 - accuracy: 0.9500\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0940 - accuracy: 0.9500\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0927 - accuracy: 0.9500\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0711 - accuracy: 0.9500\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0935 - accuracy: 0.9500\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0354 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0892 - accuracy: 0.9500\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0351 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0183 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0878 - accuracy: 0.9500\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0425 - accuracy: 0.9500\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0550 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0746 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0110 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0856 - accuracy: 0.9500\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0566 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0750 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.0477e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6614e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.2965e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0355 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.8866e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.3050e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8125e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0044e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.9255e-05 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7393e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.0526e-04 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.0075e-04 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.0419e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1438e-04 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7120e-04 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.4395e-04 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.1288e-04 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.7840e-04 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5603e-04 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7864e-05 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.8191e-04 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3738e-04 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0339 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.0598e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7966e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9806e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.7473e-04 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2539e-04 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.5407e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5796e-04 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.2674e-04 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3648e-04 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.4417e-04 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.0232e-04 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6994e-04 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.1215e-04 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.1341e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.4570e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.3171e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6684e-04 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5261e-04 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0506e-04 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4573e-04 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7768e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8029e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0114 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1791e-04 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0308e-04 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7066e-04 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1114e-04 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.9315e-04 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6951e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1908e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.9423e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6011e-04 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0679 - accuracy: 0.9500\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.4639e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.9177e-04 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.0767e-04 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.7103e-04 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3740e-04 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7301e-04 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.0364e-04 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.0251e-04 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.4463e-04 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.3733e-04 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0099e-04 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0172 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.5040e-04 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.3510e-04 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2939e-04 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.3245e-05 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.4311e-04 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8447e-04 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.9618e-04 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.4788e-04 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.4907e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6947e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0284 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1281e-04 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.9992e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.0155e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.8517e-04 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.9755e-04 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.6076e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.1255e-04 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3609e-04 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.4670e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f56b13df510>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ourtext(text):\n",
        "   tokens=nltk.word_tokenize(text)\n",
        "   tokens=[lm.lemmatize(word) for word in tokens]\n",
        "   return tokens\n",
        "\n",
        "def wordbag(text,vocab):\n",
        "    tokens=ourtext(text)\n",
        "    bagOfwords=[0]*len(vocab)\n",
        "    for w in tokens:\n",
        "        for idx,word in enumerate(vocab):\n",
        "            if word==w:\n",
        "                bagOfwords[idx]=1\n",
        "    return np.array(bagOfwords)\n",
        "\n",
        "def Pclass(text,vocab,labels):\n",
        "    bagOfwords=wordbag(text,vocab)\n",
        "    result=model.predict(np.array([bagOfwords]))[0]\n",
        "    thresh=0.2\n",
        "    yp=[[idx,res] for idx,res in enumerate(result) if res>thresh]\n",
        "    yp.sort(key=lambda x:x[1],reverse=True)\n",
        "    newlist=[]\n",
        "    for r in yp:\n",
        "        newlist.append(labels[r[0]])\n",
        "    return newlist\n",
        "\n",
        "def get_response(firstlist,firstJson):\n",
        "    tag=firstlist[0]\n",
        "    listofIntents=firstJson[\"intents\"]\n",
        "    for i in listofIntents:\n",
        "        if i[\"tag\"]==tag:\n",
        "            result=random.choice(i[\"responses\"])\n",
        "            break\n",
        "    return result"
      ],
      "metadata": {
        "id": "4lLN5Vcnjb_B"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    new_msg=input(\"\")\n",
        "    intents=Pclass(new_msg,newWords,classes)\n",
        "    result=get_response(intents,data)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "B0_z797OjfQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb63b8fc-3434-46ea-d5bb-b6ba14fc4549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "Howdy partner!\n",
            "how old are u\n",
            "I am 20 years old\n",
            "what's ur name\n",
            "I'm John\n",
            "Tell me something funny\n",
            "Why did the hipster burn his mouth? He drank the coffee before it was cool.\n",
            "What are you doing this weekend\n",
            "I am available all week\n",
            "bye\n",
            "Bye\n"
          ]
        }
      ]
    }
  ]
}